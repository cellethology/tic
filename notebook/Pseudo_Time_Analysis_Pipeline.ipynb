{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b0e42f",
   "metadata": {},
   "source": [
    "# Pseudo-Time Analysis Pipeline\n",
    "\n",
    "## Introduction\n",
    "This notebook implements a pipeline for analyzing pseudo-time in cellular graph datasets. The pipeline includes the following steps:\n",
    "\n",
    "1. **Dataset Initialization**\n",
    "2. **Subgraph Sampling**\n",
    "3. **Embedding Preparation**\n",
    "4. **Pseudo-Time Analysis**\n",
    "5. **Visualization and Output**\n",
    "\n",
    "All configurations are controlled through the `Config` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2a27e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adapters.space_gm_adapter import CustomSubgraphSampler\n",
    "import os\n",
    "from core.pseudotime_analysis import aggregate_biomarker_by_pseudotime_with_overlap, perform_pseudotime_analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from spacegm import CellularGraphDataset, GNN_pred\n",
    "\n",
    "from spacegm.embeddings_analysis import (\n",
    "    get_embedding,\n",
    "    get_composition_vector,\n",
    "    dimensionality_reduction_combo\n",
    ")\n",
    "\n",
    "from utils.data_transform import normalize\n",
    "from utils.visualization import plot_biomarker_vs_pseudotime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c38410",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Modify the parameters in the `Config` class to customize the pipeline for your dataset and analysis needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89fbde22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        # Paths\n",
    "        self.data_root = \"/root/autodl-tmp/Data/Space-Gm/Processed_Dataset/UPMC\"\n",
    "        self.output_dir = \"/root/TIC/data/embedding_analysis/pseudotime_analysis/test\"\n",
    "        self.model_path = \"/root/autodl-tmp/Data/Space-Gm/Processed_Dataset/UPMC/model/graph_level/GIN-primary_outcome-0/model_save_6.pt\"\n",
    "        self.device = 'cuda:0'\n",
    "\n",
    "        # Dataset parameters\n",
    "        self.dataset_kwargs = {\n",
    "            'raw_folder_name': 'graph',\n",
    "            'processed_folder_name': 'tg_graph',\n",
    "            'node_features': [\"cell_type\", \"SIZE\", \"biomarker_expression\", \"neighborhood_composition\", \"center_coord\"],\n",
    "            'edge_features': [\"edge_type\", \"distance\"],\n",
    "            'cell_type_mapping': None,\n",
    "            'cell_type_freq': None,\n",
    "            'biomarkers': [\"ASMA\", \"PANCK\", \"VIMENTIN\", \"PODOPLANIN\"],\n",
    "            'subgraph_size': 3,\n",
    "            'subgraph_source': 'chunk_save',\n",
    "            'subgraph_allow_distant_edge': True,\n",
    "            'subgraph_radius_limit': 55 * 3 + 35,\n",
    "            'biomarker_expression_process_method': \"linear\",\n",
    "            'biomarker_expression_lower_bound': 0,\n",
    "            'biomarker_expression_upper_bound': 18,\n",
    "            'neighborhood_size': 10,\n",
    "        }\n",
    "\n",
    "        # Sampler parameters\n",
    "        self.sampler_kwargs = {\n",
    "            'total_samples': 1000,\n",
    "            'cell_type': 9,\n",
    "            'region_id': None,\n",
    "            'batch_size': 64,\n",
    "            'num_workers': 8,\n",
    "            'include_node_info': True,\n",
    "            'random_seed': 42,\n",
    "        }\n",
    "\n",
    "        # Pseudo-time analysis parameters\n",
    "        self.embedding_keys = [\n",
    "            \"expression_vectors\", \n",
    "            \"composition_vectors\", \n",
    "            \"node_embeddings\", \n",
    "            \"graph_embeddings\",\n",
    "            \"composition_vectors+expression_vectors\",\n",
    "            \"node_embeddings+expression_vectors\",\n",
    "            \"graph_embeddings+composition_vectors\"\n",
    "        ]\n",
    "        self.start_nodes = [0, 1]\n",
    "        self.biomarkers = [\"ASMA\", \"PANCK\", \"VIMENTIN\", \"PODOPLANIN\"]\n",
    "        self.show_plots = True\n",
    "        self.num_bins = 100\n",
    "        self.use_bins = True\n",
    "        self.plotting_transform = [normalize]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dfffc1",
   "metadata": {},
   "source": [
    "## Step 1: Dataset Initialization\n",
    "Load and preprocess the cellular graph dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0030a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_dataset(root_path, dataset_kwargs):\n",
    "    return CellularGraphDataset(root_path, **dataset_kwargs)\n",
    "\n",
    "config = Config()\n",
    "dataset = initialize_dataset(config.data_root, config.dataset_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39897cdc",
   "metadata": {},
   "source": [
    "## Step 2: Subgraph Sampling\n",
    "Sample subgraphs based on specific conditions like cell type or region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce384a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_sampler(dataset, sampler_kwargs):\n",
    "    return CustomSubgraphSampler(dataset, **sampler_kwargs)\n",
    "\n",
    "sampler = initialize_sampler(dataset, config.sampler_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8c894c",
   "metadata": {},
   "source": [
    "## Step 3: Embedding Preparation\n",
    "Prepare and concatenate embeddings for the sampled subgraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83e82eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_embeddings(dataset, sampler, model_path, device, embedding_keys):\n",
    "    \"\"\"Prepare and add selected embeddings to subgraphs.\"\"\"\n",
    "    pyg_subgraphs = sampler.get_subgraph_objects()\n",
    "\n",
    "    embeddings_dict = {}\n",
    "\n",
    "    if \"composition_vectors\" in embedding_keys:\n",
    "        composition_vectors = [\n",
    "            get_composition_vector(data, n_cell_types=len(dataset.cell_type_mapping))\n",
    "            for data in pyg_subgraphs\n",
    "        ]\n",
    "        sampler.add_kv_to_sampled_subgraphs(composition_vectors, key=\"composition_vectors\")\n",
    "        embeddings_dict[\"composition_vectors\"] = composition_vectors\n",
    "\n",
    "    if \"node_embeddings\" in embedding_keys or \"graph_embeddings\" in embedding_keys:\n",
    "        model_kwargs = {\n",
    "            'num_layer': dataset.subgraph_size,\n",
    "            'num_node_type': len(dataset.cell_type_mapping) + 1,\n",
    "            'num_feat': dataset[0].x.shape[1] - 1,\n",
    "            'emb_dim': 512,\n",
    "            'num_node_tasks': 0,\n",
    "            'num_graph_tasks': 1,\n",
    "            'node_embedding_output': 'last',\n",
    "            'drop_ratio': 0.25,\n",
    "            'graph_pooling': \"max\",\n",
    "            'gnn_type': 'gin',\n",
    "        }\n",
    "\n",
    "        model = GNN_pred(**model_kwargs).to(device)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "        node_embeddings, graph_embeddings, _ = get_embedding(model, pyg_subgraphs, device)\n",
    "\n",
    "        if \"node_embeddings\" in embedding_keys:\n",
    "            sampler.add_kv_to_sampled_subgraphs(node_embeddings, key=\"node_embeddings\")\n",
    "            embeddings_dict[\"node_embeddings\"] = node_embeddings\n",
    "\n",
    "        if \"graph_embeddings\" in embedding_keys:\n",
    "            sampler.add_kv_to_sampled_subgraphs(graph_embeddings, key=\"graph_embeddings\")\n",
    "            embeddings_dict[\"graph_embeddings\"] = graph_embeddings\n",
    "\n",
    "    if \"expression_vectors\" in embedding_keys:\n",
    "        def extract_expression_vector(subgraph_dict):\n",
    "            node_info = subgraph_dict.get(\"node_info\", {})\n",
    "            biomarker_expressions = node_info.get(\"biomarker_expression\", {})\n",
    "            return np.array(list(biomarker_expressions.values()))\n",
    "\n",
    "        expression_vectors = [\n",
    "            extract_expression_vector(subgraph) for subgraph in sampler.get_all_sampled_subgraphs()\n",
    "        ]\n",
    "        sampler.add_kv_to_sampled_subgraphs(expression_vectors, key=\"expression_vectors\")\n",
    "        embeddings_dict[\"expression_vectors\"] = expression_vectors\n",
    "\n",
    "    # Handle concatenated embeddings\n",
    "    for key in embedding_keys:\n",
    "        if \"+\" in key:\n",
    "            components = key.split(\"+\")\n",
    "            concatenated_embeddings = [\n",
    "                np.concatenate([embeddings_dict[comp][i] for comp in components if comp in embeddings_dict], axis=None)\n",
    "                for i in range(len(pyg_subgraphs))\n",
    "            ]\n",
    "            sampler.add_kv_to_sampled_subgraphs(concatenated_embeddings, key=key)\n",
    "\n",
    "    return sampler\n",
    "\n",
    "\n",
    "sampler = prepare_embeddings(dataset, sampler, config.model_path, config.device, config.embedding_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62aead30",
   "metadata": {},
   "source": [
    "## Step 4: Pseudo-Time Analysis\n",
    "Perform dimensionality reduction, clustering, and compute pseudo-time for selected start nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f87630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pseudotime_to_csv(sampled_subgraphs, output_path):\n",
    "    \"\"\"Save region ID, cell ID, and pseudotime to a CSV file.\"\"\"\n",
    "    data = [\n",
    "        {\n",
    "            \"region_id\": subgraph[\"region_id\"],\n",
    "            \"cell_id\": subgraph[\"cell_id\"],\n",
    "            \"pseudotime\": subgraph.get(\"pseudotime\", np.nan)\n",
    "        }\n",
    "        for subgraph in sampled_subgraphs\n",
    "    ]\n",
    "    df = pd.DataFrame(data)\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Pseudotime data saved to {output_path}\")\n",
    "    \n",
    "def perform_pseudo_time_analysis_pipeline(config, sampler):\n",
    "    \"\"\"Run pseudo-time analysis pipeline.\"\"\"\n",
    "    sampled_subgraph_dicts = sampler.get_all_sampled_subgraphs()\n",
    "\n",
    "    for embedding_key in config.embedding_keys:\n",
    "        embeddings = np.array([subgraph.get(embedding_key) for subgraph in sampled_subgraph_dicts])\n",
    "\n",
    "        # Dimensionality reduction and clustering\n",
    "        pca_embs, umap_embs, cluster_labels, _ = dimensionality_reduction_combo(\n",
    "            embeddings, n_pca_components=10, cluster_method='kmeans', n_clusters=2, seed=42\n",
    "        )\n",
    "\n",
    "        # Attach cluster labels to subgraphs\n",
    "        for i, subgraph in enumerate(sampled_subgraph_dicts):\n",
    "            subgraph[\"cluster_label\"] = cluster_labels[i]\n",
    "\n",
    "        # Perform pseudo-time analysis for each start node\n",
    "        for start_node in config.start_nodes:\n",
    "            output_dir = os.path.join(config.output_dir, embedding_key, f\"start_node_{start_node}\")\n",
    "            output_path = os.path.join(output_dir, \"pseudotime.csv\")\n",
    "\n",
    "            pseudotime_results = perform_pseudotime_analysis(\n",
    "                labels=cluster_labels,\n",
    "                umap_embs=umap_embs,\n",
    "                output_dir=output_dir,\n",
    "                start=start_node,\n",
    "                show_plots=config.show_plots\n",
    "            )\n",
    "            sampler.add_kv_to_sampled_subgraphs(pseudotime_results, key=\"pseudotime\")\n",
    "\n",
    "            # Save pseudotime data\n",
    "            save_pseudotime_to_csv(sampled_subgraph_dicts, output_path)\n",
    "\n",
    "            # Aggregate biomarker data\n",
    "            aggregated_data = aggregate_biomarker_by_pseudotime_with_overlap(\n",
    "                sampled_subgraph_dicts, config.biomarkers, num_bins=config.num_bins, use_bins=config.use_bins\n",
    "            )\n",
    "\n",
    "            # Plot and save biomarker trends\n",
    "            plot_biomarker_vs_pseudotime(aggregated_data, output_dir,method=embedding_key,transforms=config.plotting_transform, use_bins=config.use_bins)\n",
    "\n",
    "perform_pseudo_time_analysis_pipeline(config, sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e5d555",
   "metadata": {},
   "source": [
    "## Step 5: Visualization and Output\n",
    "Visualize biomarker trends across pseudo-time and save results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
